{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare LLama Providers\n",
    "\n",
    "Run evaluations on a few prompts for llama3.1 70B across several providers and comprate results to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connections...\n",
      "OpenRouter connection successful.\n",
      "🍩 https://wandb.ai/tuminha/compare-llamas/r/call/01924d97-e674-7761-9887-7459c06e93fe\n",
      "Groq connection successful.\n",
      "Together connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Test connection to all the API keys\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "from together import Together\n",
    "import requests\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def test_openrouter_connection():\n",
    "    api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"OpenRouter API key not found.\")\n",
    "        return\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(\"https://openrouter.ai/api/v1/models\", headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\"OpenRouter connection successful.\")\n",
    "    else:\n",
    "        print(f\"OpenRouter connection failed. Status code: {response.status_code}\")\n",
    "\n",
    "def test_groq_connection():\n",
    "    api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Groq API key not found.\")\n",
    "        return\n",
    "    \n",
    "    groq_client = Groq(api_key=api_key)\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            model=\"mixtral-8x7b-32768\"  # Using a model supported by Groq\n",
    "        )\n",
    "        if response.choices[0].message.content:\n",
    "            print(\"Groq connection successful.\")\n",
    "        else:\n",
    "            print(\"Groq connection failed. No content in response.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Groq connection failed. Error: {str(e)}\")\n",
    "\n",
    "def test_together_connection():\n",
    "    api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Together API key not found.\")\n",
    "        return\n",
    "    \n",
    "    together_client = Together(api_key=api_key)\n",
    "    try:\n",
    "        together_client.models.list()\n",
    "        print(\"Together connection successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Together connection failed. Error: {str(e)}\")\n",
    "\n",
    "# Run connection tests\n",
    "print(\"Testing API connections...\")\n",
    "test_openrouter_connection()\n",
    "test_groq_connection()\n",
    "test_together_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Installing packages\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/sortedcontainers-2.4.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/pdfminer.six-20200121-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/grobid_client_python-0.0.7-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "✅ Packages installed\n"
     ]
    }
   ],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "print('⏳ Installing packages')\n",
    "%pip install -q weave set-env-colab-kaggle-dotenv tqdm ipywidgets requests groq together\n",
    "print('✅ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/sortedcontainers-2.4.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/pdfminer.six-20200121-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/grobid_client_python-0.0.7-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-community in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (1.26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain-community) (2.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: anyio in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/sortedcontainers-2.4.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/pdfminer.six-20200121-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages/grobid_client_python-0.0.7-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: together in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (3.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (0.2.0)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (1.26.0)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (16.1.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (2.31.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (13.9.1)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (4.66.5)\n",
      "Requirement already satisfied: typer<0.13,>=0.9 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.13.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/franciscoteixeirabarbosa/anaconda3/envs/streamlitapp/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Together client\n",
    "import os\n",
    "from together import Together\n",
    "\n",
    "# Initialize Together client\n",
    "together_client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from set_env import set_env\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "from together import Together\n",
    "import weave\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "set_env(\"OPENROUTER_API_KEY\")\n",
    "set_env(\"GROQ_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "\n",
    "groqclient = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: tuminha.\n",
      "View Weave data at https://wandb.ai/tuminha/compare-llamas/weave\n",
      "✅ Weave models created\n"
     ]
    }
   ],
   "source": [
    "# Initialize Weave project\n",
    "weave.init('compare-llamas')\n",
    "\n",
    "# Define a base LlamaModel class using Weave\n",
    "class LlamaModel(weave.Model):\n",
    "    provider: str  # Provider attribute to specify the API provider\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"provider\": {\n",
    "                \"order\": [self.provider],\n",
    "                \"allow_fallbacks\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Make the API request and return the response\n",
    "        response = self.make_openrouter_request(data)\n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "    @weave.op()\n",
    "    def make_openrouter_request(self, data):\n",
    "        # Make a POST request to the OpenRouter API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {os.environ['OPENROUTER_API_KEY']}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json=data\n",
    "            )\n",
    "            response.raise_for_status()  # Raise an exception for bad responses\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"API request failed: {str(e)}\")\n",
    "\n",
    "# Define a GroqModel class using Weave\n",
    "class GroqModel(weave.Model):\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        # Make a request to the Groq API\n",
    "        response = groqclient.chat.completions.create(\n",
    "            model='llama-3.1-70b-versatile',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            seed=123123\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "# Define a TogetherModel class using Weave\n",
    "class TogetherModel(weave.Model):\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        # Make a request to the Together API\n",
    "        response = self.make_together_request(prompt)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @weave.op()\n",
    "    def make_together_request(self, prompt):\n",
    "        # Create a chat completion using the Together API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "# Create instances of LlamaModel for different providers\n",
    "octoai_llama = LlamaModel(provider='OctoAI', name='OctoAILLa_LLama3.1_70B')\n",
    "novitaai_llama = LlamaModel(provider='Novita', name='NovitaAI_LLaMa3.1_70B')\n",
    "deepinfra_llama = LlamaModel(provider='DeepInfra', name='DeepInfra_LLaMa3.1_70B')\n",
    "fireworks_llama = LlamaModel(provider='Fireworks', name='Fireworks_LLaMa3.1_70B')\n",
    "\n",
    "# Create instances of GroqModel and TogetherModel\n",
    "groq_llama = GroqModel(name='Groq_LLaMa3.1_70B')\n",
    "together_llama = TogetherModel(name='Together_LLaMa3.1_70B')\n",
    "\n",
    "print(\"✅ Weave models created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a dataset of quirky prompts and potentially their answers \n",
    "from weave import Dataset\n",
    "\n",
    "# Define a dataset of quirky prompts and their corresponding rubrics\n",
    "quirky_prompts = Dataset(\n",
    "    name=\"my_llama_quirky_prompts\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"question\": \"Give me 10 sentences that end in the word \\\"apple\\\"\",\n",
    "            \"rubric\": \"all sentences must end with the word apple\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Answer with the number of legs about the following statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that\",\n",
    "            \"rubric\": \"Answer must be 5 or five\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Yam (a boy) has 4 sisters. Each sister has 3 brothers. How many brothers does Yam have? Let's think step by step.\",\n",
    "            \"rubric\": \"Answer must indicate that Yam has 2 brothers\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"You have five apples today, you ate two apples yesterday so how many apples do you have today? Provide a logical answer.\",\n",
    "            \"rubric\": \"Answer must be five and explain that yesterdays action have no bearing on todays apple quantity\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Which number is bigger: 9.11 or 9.9?\",\n",
    "            \"rubric\": \"Answer should conclude that 9.9 is bigger\" \n",
    "        },\n",
    "        {\n",
    "            \"question\": \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts\",\n",
    "            \"rubric\": \"Answer must state that it would take the same amount of time\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"There are three sisters in a room. Anna is reading a book. Alice is playing a match of chess against someone in the room. What is the third sister, Amanda, doing?\",\n",
    "            \"rubric\": \"Playing chess with Alice\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"\"\"Determine all triples (x, y, z) of real numbers that are solutions to the following\n",
    "system of equations:\n",
    "log9 x + 10g9 y + 10g3 z = 2\n",
    "log 16 x + log4 y + log16 z = 1\n",
    "log5 x + log25 y + log25 z = 0\n",
    "\"\"\",\n",
    "            \"rubric\": \"IDK the answer to this one\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Detailed explanation of the code:\n",
    "\n",
    "# 1. We import the Dataset class from the weave module.\n",
    "#    This class is used to create a structured dataset that can be used for evaluation.\n",
    "\n",
    "# 2. We create a Dataset object named \"quirky_prompts\" with the following properties:\n",
    "#    - name: A string identifier for the dataset (\"my_llama_quirky_prompts\")\n",
    "#    - rows: A list of dictionaries, where each dictionary represents a prompt\n",
    "\n",
    "# 3. Each row in the dataset contains two key-value pairs:\n",
    "#    - \"question\": The quirky prompt or question to be answered\n",
    "#    - \"rubric\": The criteria or expected answer for evaluating the model's response\n",
    "\n",
    "# 4. The dataset contains 8 different prompts, each designed to test various aspects of language model capabilities:\n",
    "#    - Sentence generation with specific endings\n",
    "#    - Logical reasoning and arithmetic\n",
    "#    - Understanding context and implicit information\n",
    "#    - Temporal logic\n",
    "#    - Numerical comparison\n",
    "#    - Problem-solving with irrelevant information\n",
    "#    - Inference from given information\n",
    "#    - Complex mathematical problem (intentionally difficult)\n",
    "\n",
    "# 5. This dataset will be used later in the evaluation process to test different language models\n",
    "#    and assess their performance on these quirky and challenging prompts.\n",
    "\n",
    "# Explanation of rubrics:\n",
    "# Rubrics are evaluation criteria or guidelines used to assess the quality or correctness of a response.\n",
    "# In this context, each question in our dataset is accompanied by a rubric that specifies what constitutes\n",
    "# a correct or acceptable answer. Rubrics serve several important purposes:\n",
    "\n",
    "# 1. Consistency: They ensure that all responses are evaluated using the same criteria.\n",
    "# 2. Objectivity: They help reduce subjectivity in the evaluation process.\n",
    "# 3. Clarity: They provide clear expectations for what a good answer should include.\n",
    "# 4. Feedback: They can be used to provide specific feedback on where a response falls short.\n",
    "\n",
    "# We use rubrics in this dataset to:\n",
    "# - Guide the evaluation of model responses\n",
    "# - Determine if a model has correctly understood and answered the quirky prompts\n",
    "# - Measure the model's ability to handle tricky or unconventional questions\n",
    "# - Assess the model's reasoning capabilities and attention to detail\n",
    "\n",
    "# The rubrics in this dataset are relatively simple, often just stating the correct answer or key points\n",
    "# that should be included. In more complex evaluation scenarios, rubrics could be more detailed,\n",
    "# including scoring criteria or multiple levels of correctness.\n",
    "\n",
    "# Additional examples that could be added to the dataset:\n",
    "\n",
    "# {\n",
    "#     \"question\": \"If you're running a race and you pass the person in second place, what place are you in now?\",\n",
    "#     \"rubric\": \"Answer should be second place\"\n",
    "# },\n",
    "# {\n",
    "#     \"question\": \"A rooster lays exactly one egg every day. How many eggs will it have laid in one week?\",\n",
    "#     \"rubric\": \"Answer should explain that roosters don't lay eggs\"\n",
    "# },\n",
    "# {\n",
    "#     \"question\": \"What's the next number in this sequence: 1, 11, 21, 1211, 111221, ...\",\n",
    "#     \"rubric\": \"Answer should be 312211 (look-and-say sequence)\"\n",
    "# }\n",
    "\n",
    "# These additional examples would further test the model's ability to handle trick questions,\n",
    "# common misconceptions, and pattern recognition. The rubrics for these examples follow the same\n",
    "# principle of providing clear criteria for what constitutes a correct answer, allowing for\n",
    "# consistent evaluation across different models or evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DeepInfra_LLaMa3.1_70B...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.996799170970917</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m7.996799170970917\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/compare-llamas/r/call/01924c11-29e7-7ed3-a9da-a557a563f438\n",
      "Evaluating Fireworks_LLaMa3.1_70B...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.270969808101654</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.270969808101654\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/compare-llamas/r/call/01924c11-a827-7df3-9e7f-7374001ad308\n",
      "Evaluating Groq_LLaMa3.1_70B...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m8\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'has_response'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.8930029571056366</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'has_response'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.8930029571056366\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/compare-llamas/r/call/01924c11-d517-7e21-8b56-048d1256c17a\n"
     ]
    }
   ],
   "source": [
    "# Get all instantiated models\n",
    "# models = [octoai_llama, together_llama]\n",
    "models = [deepinfra_llama, fireworks_llama, groq_llama]\n",
    "\n",
    "# Define our scoring functions\n",
    "# The '@' symbol in Python is used for decorators. Decorators are a way to modify or enhance functions\n",
    "# without changing their source code directly. In this case, @weave.op() is a decorator that likely\n",
    "# registers this function with the Weave framework for use in evaluations.\n",
    "@weave.op()\n",
    "def has_response(rubric: str, model_output: dict) -> dict:\n",
    "    # This function checks if the model output is not None and returns a dictionary\n",
    "    # indicating whether there's a response or not\n",
    "    return {'has_response': model_output is not None}\n",
    "\n",
    "# Define the preprocess_model_input function\n",
    "def preprocess_model_input(row):\n",
    "    # This function takes a row from the dataset and formats it as input for the model\n",
    "    # It extracts the 'question' field and puts it in a dictionary under the key 'prompt'\n",
    "    return {'prompt': row['question']}\n",
    "\n",
    "# Define the evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name='quirky_prompts_eval',  # Name of the evaluation\n",
    "    dataset=quirky_prompts,      # Dataset to use for evaluation\n",
    "    trials=1,                    # Number of trials to run\n",
    "    scorers=[\n",
    "        has_response             # List of scoring functions to use\n",
    "    ],\n",
    "    preprocess_model_input=preprocess_model_input  # Function to preprocess input\n",
    ")\n",
    "\n",
    "# Run evaluation for each model\n",
    "results = {}\n",
    "for model in models:\n",
    "    print(f\"Evaluating {model.name}...\")\n",
    "    # The 'await' keyword suggests this is running in an asynchronous context\n",
    "    # It evaluates the model using the defined evaluation setup\n",
    "    result = await evaluation.evaluate(model)\n",
    "    # Store the result for each model in the results dictionary\n",
    "    results[model.name] = result\n",
    "\n",
    "\"\"\"\n",
    "Here's another example of how the '@' decorator works in Python:\n",
    "\n",
    "@timer\n",
    "def slow_function():\n",
    "    time.sleep(2)\n",
    "    print(\"Function complete\")\n",
    "\n",
    "In this example, '@timer' is a decorator that might be defined like this:\n",
    "\n",
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Function {func.__name__} took {end - start} seconds to run\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "When 'slow_function' is called, it will actually run the 'wrapper' function defined in 'timer'.\n",
    "This wrapper will time how long the original function takes to run, print that information,\n",
    "and then return the result of the original function. This allows us to add timing functionality\n",
    "to any function simply by adding the '@timer' decorator, without changing the function itself.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlitapp",
   "language": "python",
   "name": "streamlitapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
