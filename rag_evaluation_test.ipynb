{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./wandb/lib/python3.9/site-packages (24.2)\n",
      "Requirement already satisfied: setuptools in ./wandb/lib/python3.9/site-packages (56.0.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 56.0.0\n",
      "    Uninstalling setuptools-56.0.0:\n",
      "      Successfully uninstalled setuptools-56.0.0\n",
      "Successfully installed setuptools-75.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./wandb/lib/python3.9/site-packages (from groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./wandb/lib/python3.9/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./wandb/lib/python3.9/site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./wandb/lib/python3.9/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./wandb/lib/python3.9/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./wandb/lib/python3.9/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./wandb/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./wandb/lib/python3.9/site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in ./wandb/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./wandb/lib/python3.9/site-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./wandb/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./wandb/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./wandb/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "Using cached groq-0.11.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in ./wandb/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./wandb/lib/python3.9/site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp39-cp39-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./wandb/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: openai in ./wandb/lib/python3.9/site-packages (1.51.0)\n",
      "Requirement already satisfied: weave in ./wandb/lib/python3.9/site-packages (0.51.12)\n",
      "Collecting asyncio\n",
      "  Using cached asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting dotenv\n",
      "  Using cached dotenv-0.0.5.tar.gz (2.4 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting distribute\n",
      "  \u001b[31m   \u001b[0m   Using cached distribute-0.7.3.zip (145 kB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help [cmd1 cmd2 ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help-commands\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py cmd --help\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m error: invalid command 'dist_info'\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all needed packages\n",
    "%pip install numpy openai weave asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load API from .env file for open AI and also Weights & Biases\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "if not openai_api_key or not wandb_api_key:\n",
    "    raise ValueError(\"API keys not found in .env file\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG database created successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import weave\n",
    "from weave import Model\n",
    "import numpy as np\n",
    "import json\n",
    "import asyncio \n",
    "import faiss\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "articles = [\n",
    "    \"Novo Nordisk and Eli Lilly rival soars 32 percent after promising weight loss drug results Shares of Denmarks Zealand Pharma shot 32 percent higher in morning trade, after results showed success in its liver disease treatment survodutide, which is also on trial as a drug to treat obesity. The trial \\\"tells us that the 6mg dose is safe, which is the top dose used in the ongoing [Phase 3] obesity trial too,\\\" one analyst said in a note. The results come amid feverish investor interest in drugs that can be used for weight loss.\",\n",
    "    \"Berkshire shares jump after big profit gain as Buffetts conglomerate nears $1 trillion valuation Berkshire Hathaway shares rose on Monday after Warren Buffetts conglomerate posted strong earnings for the fourth quarter over the weekend. Berkshires Class A and B shares jumped more than 1.5%, each. Class A shares are higher by more than 17% this year, while Class B have gained more than 18%. Berkshire was last valued at $930.1 billion, up from $905.5 billion where it closed on Friday, according to FactSet. Berkshire on Saturday posted fourth-quarter operating earnings of $8.481 billion, about 28 percent higher than the $6.625 billion from the year-ago period, driven by big gains in its insurance business. Operating earnings refers to profits from businesses across insurance, railroads and utilities. Meanwhile, Berkshires cash levels also swelled to record levels. The conglomerate held $167.6 billion in cash in the fourth quarter, surpassing the $157.2 billion record the conglomerate held in the prior quarter.\",\n",
    "    \"Highmark Health says its combining tech from Google and Epic to give doctors easier access to information Highmark Health announced it is integrating technology from Google Cloud and the health-care software company Epic Systems. The integration aims to make it easier for both payers and providers to access key information they need, even if its stored across multiple points and formats, the company said. Highmark is the parent company of a health plan with 7 million members, a provider network of 14 hospitals and other entities\",\n",
    "    \"Rivian and Lucid shares plunge after weak EV earnings reports Shares of electric vehicle makers Rivian and Lucid fell Thursday after the companies reported stagnant production in their fourth-quarter earnings after the bell Wednesday. Rivian shares sank about 25 percent, and Lucids stock dropped around 17 percent. Rivian forecast it will make 57,000 vehicles in 2024, slightly less than the 57,232 vehicles it produced in 2023. Lucid said it expects to make 9,000 vehicles in 2024, more than the 8,428 vehicles it made in 2023.\",\n",
    "    \"Mauritius blocks Norwegian cruise ship over fears of a potential cholera outbreak Local authorities on Sunday denied permission for the Norwegian Dawn ship, which has 2,184 passengers and 1,026 crew on board, to access the Mauritius capital of Port Louis, citing \\\"potential health risks.\\\" The Mauritius Ports Authority said Sunday that samples were taken from at least 15 passengers on board the cruise ship. A spokesperson for the U.S.-headquartered Norwegian Cruise Line Holdings said Sunday that 'a small number of guests experienced mild symptoms of a stomach-related illness' during Norwegian Dawns South Africa voyage.\",\n",
    "    \"Intuitive Machines lands on the moon in historic first for a U.S. company Intuitive Machines Nova-C cargo lander, named Odysseus after the mythological Greek hero, is the first U.S. spacecraft to soft land on the lunar surface since 1972. Intuitive Machines is the first company to pull off a moon landing — government agencies have carried out all previously successful missions. The companys stock surged in extended trading Thursday, after falling 11 percent in regular trading.\",\n",
    "    \"Lunar landing photos: Intuitive Machines Odysseus sends back first images from the moon Intuitive Machines cargo moon lander Odysseus returned its first images from the surface. Company executives believe the lander caught its landing gear sideways in the moons surface while touching down and tipped over. Despite resting on its side, the companys historic IM-1 mission is still operating on the moon.\",\n",
    "]\n",
    "\n",
    "def create_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "def create_rag_database(articles):\n",
    "    embeddings = create_embeddings(articles)\n",
    "    \n",
    "    dimension = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    faiss.write_index(index, \"article_index.faiss\")\n",
    "    \n",
    "    with open(\"articles.json\", \"w\") as f:\n",
    "        json.dump(articles, f)\n",
    "\n",
    "    print(\"RAG database created successfully.\")\n",
    "\n",
    "create_rag_database(articles)\n",
    "\n",
    "def query_rag_database(query):\n",
    "    index = faiss.read_index(\"article_index.faiss\")\n",
    "    \n",
    "    with open(\"articles.json\", \"r\") as f:\n",
    "        stored_articles = json.load(f)\n",
    "    \n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    k = 1\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    most_relevant_doc = stored_articles[indices[0][0]]\n",
    "    \n",
    "    return most_relevant_doc\n",
    "\n",
    "class RAGModel(Model):\n",
    "    system_message: str\n",
    "    model_name: str = \"gpt-4o\"\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str) -> dict:\n",
    "        context = query_rag_database(question)\n",
    "        client = OpenAI()\n",
    "        query = f\"\"\"Use the following information to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
    "        Context:\n",
    "        \\\"\\\"\\\"\n",
    "        {context}\n",
    "        \\\"\\\"\\\"\n",
    "        Question: {question}\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"text\"},\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        return {'answer': answer, 'context': context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a RAG App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/rag-qa/r/call/01925065-a9d9-7773-8249-4c385c039b32\n",
      "{'answer': \"The significant result reported about Zealand Pharma's obesity trial was the success in its liver disease treatment survodutide, which is also on trial as a drug to treat obesity. The trial indicated that the 6mg dose is safe, which is the top dose used in the ongoing Phase 3 obesity trial.\", 'context': 'Novo Nordisk and Eli Lilly rival soars 32 percent after promising weight loss drug results Shares of Denmarks Zealand Pharma shot 32 percent higher in morning trade, after results showed success in its liver disease treatment survodutide, which is also on trial as a drug to treat obesity. The trial \"tells us that the 6mg dose is safe, which is the top dose used in the ongoing [Phase 3] obesity trial too,\" one analyst said in a note. The results come amid feverish investor interest in drugs that can be used for weight loss.'}\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import weave\n",
    "from weave import Model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def create_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "def query_rag_database(query):\n",
    "    index = faiss.read_index(\"article_index.faiss\")\n",
    "    \n",
    "    with open(\"articles.json\", \"r\") as f:\n",
    "        stored_articles = json.load(f)\n",
    "    \n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    k = 1  # Number of similar documents to retrieve\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    most_relevant_doc = stored_articles[indices[0][0]]\n",
    "    \n",
    "    return most_relevant_doc\n",
    "\n",
    "class RAGModel(Model):\n",
    "    system_message: str\n",
    "    model_name: str = \"gpt-4-turbo-preview\"  # Updated to a more recent model\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str) -> dict:\n",
    "        context = query_rag_database(question)\n",
    "        client = OpenAI()\n",
    "        query = f\"\"\"Use the following information to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
    "        Context:\n",
    "        \\\"\\\"\\\"\n",
    "        {context}\n",
    "        \\\"\\\"\\\"\n",
    "        Question: {question}\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"text\"},\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        return {'answer': answer, 'context': context}\n",
    "\n",
    "weave.init('rag-qa')\n",
    "model = RAGModel(\n",
    "    system_message=\"You are an expert in finance and answer questions related to finance, financial services, and financial markets. When responding based on provided information, be sure to cite the source.\"\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "result = model.predict(\"What significant result was reported about Zealand Pharma's obesity trial?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating with an LLM Judge\n",
    "\n",
    "When there aren't simple ways to evaluate your application, one approach is to use an LLM to evaluate aspects of it. Here is an example of using an LLM judge to try to measure the context precision by prompting it to verify if the context was useful in arriving at the given answer. This prompt was augmented from the popular RAGAS framework.\n",
    "\n",
    "### Defining a scoring function\n",
    "\n",
    "As we did in the Build an Evaluation pipeline tutorial, we'll define a set of example rows to test our app against and a scoring function. Our scoring function will take one row and evaluate it. The input arguments should match with the corresponding keys in our row, so question here will be taken from the row dictionary. model_output is the output of the model. The input to the model will be taken from the example based on its input argument, so question here too. We're using async functions so they run fast in parallel. If you need a quick introduction to async, you can find one here [https://realpython.com/async-io-python/](https://realpython.com/async-io-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context_precision_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'verdict'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6660459041595459</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'context_precision_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'verdict'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m7\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.6660459041595459\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/rag-qa/r/call/01925067-22a0-7011-9d48-db03ef7c717e\n",
      "{'context_precision_score': {'verdict': {'true_count': 7, 'true_fraction': 1.0}}, 'model_latency': {'mean': 0.6660459041595459}}\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import weave\n",
    "import asyncio\n",
    "from groq import Groq\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# This allows asyncio.run() to work in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "def create_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "def query_rag_database(query):\n",
    "    index = faiss.read_index(\"article_index.faiss\")\n",
    "    \n",
    "    with open(\"articles.json\", \"r\") as f:\n",
    "        stored_articles = json.load(f)\n",
    "    \n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    k = 1  # Number of similar documents to retrieve\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    most_relevant_doc = stored_articles[indices[0][0]]\n",
    "    \n",
    "    return most_relevant_doc\n",
    "\n",
    "@weave.op()\n",
    "async def context_precision_score(question, model_output):\n",
    "    context_precision_prompt = \"\"\"Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output. \n",
    "    Output in only valid JSON format.\n",
    "\n",
    "    question: {question}\n",
    "    context: {context}\n",
    "    answer: {answer}\n",
    "    verdict: \"\"\"\n",
    "    \n",
    "    prompt = context_precision_prompt.format(\n",
    "        question=question,\n",
    "        context=model_output['context'],\n",
    "        answer=model_output['answer'],\n",
    "    )\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    response = json.loads(response_message.content)\n",
    "    return {\n",
    "        \"verdict\": int(response[\"verdict\"]) == 1,\n",
    "    }\n",
    "\n",
    "questions = [\n",
    "    {\"question\": \"What significant result was reported about Zealand Pharma's obesity trial?\"},\n",
    "    {\"question\": \"How much did Berkshire Hathaway's cash levels increase in the fourth quarter?\"},\n",
    "    {\"question\": \"What is the goal of Highmark Health's integration of Google Cloud and Epic Systems technology?\"},\n",
    "    {\"question\": \"What were Rivian and Lucid's vehicle production forecasts for 2024?\"},\n",
    "    {\"question\": \"Why was the Norwegian Dawn cruise ship denied access to Mauritius?\"},\n",
    "    {\"question\": \"Which company achieved the first U.S. moon landing since 1972?\"},\n",
    "    {\"question\": \"What issue did Intuitive Machines' lunar lander encounter upon landing on the moon?\"}\n",
    "]\n",
    "\n",
    "@weave.op()\n",
    "async def rag_groq_model(question):\n",
    "    context = query_rag_database(question)\n",
    "    \n",
    "    prompt = f\"\"\"Use the following information to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
    "    Context:\n",
    "    \\\"\\\"\\\"\n",
    "    {context}\n",
    "    \\\"\\\"\\\"\n",
    "    Question: {question}\"\"\"\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return {\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# Create the evaluation\n",
    "evaluation = weave.Evaluation(dataset=questions, scorers=[context_precision_score])\n",
    "\n",
    "# Run the evaluation\n",
    "async def run_evaluation():\n",
    "    results = await evaluation.evaluate(rag_groq_model)\n",
    "    print(results)\n",
    "\n",
    "# Run the evaluation\n",
    "asyncio.run(run_evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Scorer class\n",
    "\n",
    "In some applications we want to create custom evaluation classes - where for example a standardized LLMJudge class should be created with specific parameters (e.g. chat model, prompt), specific scoring of each row, and specific calculation of an aggregate score. In order to do that Weave defines a list of ready-to-use Scorer classes and also makes it easy to create a custom Scorer - in the following we'll see how to create a custom class CorrectnessLLMJudge(Scorer).\n",
    "\n",
    "On a high-level the steps to create custom Scorer are quite simple:\n",
    "\n",
    "Define a custom class that inherits from weave.flow.scorer.Scorer\n",
    "Overwrite the score function and add a @weave.op()if you want to track each call of the function\n",
    "this function has to define a model_output argument where the prediction of the model will be passed to. We define it as type Optional[dict] in case the mode might return \"None\".\n",
    "the rest of the arguments can either be a general Any or dict or can select specific columns from the dataset that is used to evaluate the model using the weave.Evaluate class - they have to have the exact same names as the column names or keys of a single row after being passed to preprocess_model_input if that is used.\n",
    "Optional: Overwrite the summarize function to customize the calculation of the aggregate score. By default Weave uses the weave.flow.scorer.auto_summarizefunction if you don't define a custom function.\n",
    "this function has to have a @weave.op() decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'RAGCorrectnessLLMJudge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'stderr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6492925371442523</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'RAGCorrectnessLLMJudge'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m7\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'stderr'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.6492925371442523\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/rag-qa/r/call/01925074-2551-7d53-8f0d-1bd569e44f26\n",
      "{'RAGCorrectnessLLMJudge': {'correct': {'true_count': 7, 'true_fraction': 1.0, 'stderr': 0.0}}, 'model_latency': {'mean': 0.6492925371442523}}\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave.flow.scorer import Scorer\n",
    "from typing import Optional, Any, List\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "class RAGCorrectnessLLMJudge(Scorer):\n",
    "    prompt: str = \"\"\"Given the question, context, and answer, evaluate if the answer is correct and relevant based on the provided context. \n",
    "    Respond with a JSON object containing two keys:\n",
    "    1. \"correct\": true if the answer is factually correct and relevant to the question, false otherwise.\n",
    "    2. \"explanation\": a brief explanation of your evaluation.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: {answer}\n",
    "\n",
    "    JSON response:\"\"\"\n",
    "    model_name: str = \"gpt-4-turbo-preview\"\n",
    "\n",
    "    @weave.op()\n",
    "    async def score(self, model_output: Optional[dict], question: str) -> Any:\n",
    "        \"\"\"Score the correctness of the predictions by comparing the question, context, and answer.\"\"\"\n",
    "        if model_output is None or 'answer' not in model_output or 'context' not in model_output:\n",
    "            return {\"correct\": False, \"explanation\": \"Invalid model output\"}\n",
    "\n",
    "        eval_prompt = self.prompt.format(\n",
    "            question=question,\n",
    "            context=model_output['context'],\n",
    "            answer=model_output['answer']\n",
    "        )\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        evaluation = response.choices[0].message.content\n",
    "        evaluation_dict = json.loads(evaluation)\n",
    "\n",
    "        return {\n",
    "            \"correct\": evaluation_dict[\"correct\"],\n",
    "            \"explanation\": evaluation_dict[\"explanation\"]\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def summarize(self, score_rows: List[dict]) -> Optional[dict]:\n",
    "        \"\"\"Aggregate all the scores calculated for each row by the scoring function.\"\"\"\n",
    "        valid_data = [x.get(\"correct\") for x in score_rows if x.get(\"correct\") is not None]\n",
    "        count_true = list(valid_data).count(True)\n",
    "        int_data = [int(x) for x in valid_data]\n",
    "\n",
    "        sample_mean = np.mean(int_data) if int_data else 0\n",
    "        sample_variance = np.var(int_data) if int_data else 0\n",
    "        sample_error = np.sqrt(sample_variance / len(int_data)) if int_data else 0\n",
    "\n",
    "        return {\n",
    "            \"correct\": {\n",
    "                \"true_count\": count_true,\n",
    "                \"true_fraction\": sample_mean,\n",
    "                \"stderr\": sample_error,\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create an instance of our custom scorer\n",
    "rag_correctness_judge = RAGCorrectnessLLMJudge()\n",
    "\n",
    "# Create the evaluation with only our custom scorer\n",
    "evaluation = weave.Evaluation(dataset=questions, scorers=[rag_correctness_judge])\n",
    "\n",
    "# Run the evaluation\n",
    "async def run_evaluation():\n",
    "    results = await evaluation.evaluate(rag_groq_model)\n",
    "    print(results)\n",
    "\n",
    "# Run the evaluation\n",
    "asyncio.run(run_evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pulling it all together\n",
    "\n",
    "### To get the same result for your RAG apps:\n",
    "\n",
    "Wrap LLM calls & retrieval step functions with weave.op()\n",
    "(optional) Create a Model subclass with predict function and app details\n",
    "Collect examples to evaluate\n",
    "Create scoring functions that score one example\n",
    "Use Evaluation class to run evaluations on your examples\n",
    "NOTE: Sometimes the async execution of Evaluations will trigger a rate limit on the models of OpenAI, Anthropic, etc. To prevent that you can set an environment variable to limit the amount of parallel workers e.g. WEAVE_PARALLELISM=3.\n",
    "\n",
    "Here, we show the code in it's entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m7\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context_precision_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'verdict'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.173034497669765</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'context_precision_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'verdict'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m7\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.173034497669765\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/tuminha/rag-qa/r/call/0192507d-8ab3-78f0-ba11-d1ed54f28be5\n",
      "{'context_precision_score': {'verdict': {'true_count': 7, 'true_fraction': 1.0}}, 'model_latency': {'mean': 3.173034497669765}}\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "class RAGModel(Model):\n",
    "    system_message: str\n",
    "    model_name: str = \"mixtral-8x7b-32768\"  # Groq model\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str) -> dict:\n",
    "        context = query_rag_database(question)\n",
    "        groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        query = f\"\"\"Use the following information to answer the subsequent question. If the answer cannot be found, write \"I don't know.\"\n",
    "        Context:\n",
    "        \\\"\\\"\\\"\n",
    "        {context}\n",
    "        \\\"\\\"\\\"\n",
    "        Question: {question}\"\"\"\n",
    "        response = groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "            ],\n",
    "            model=self.model_name,\n",
    "            temperature=0.0,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        return {'answer': answer, 'context': context}\n",
    "\n",
    "weave.init('rag-qa')\n",
    "model = RAGModel(\n",
    "    system_message=\"You are an expert in finance and answer questions related to finance, financial services, and financial markets. When responding based on provided information, be sure to cite the source.\"\n",
    ")\n",
    "\n",
    "@weave.op()\n",
    "async def context_precision_score(question, model_output):\n",
    "    context_precision_prompt = \"\"\"Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as \"1\" if useful and \"0\" if not with json output. \n",
    "    Output in only valid JSON format.\n",
    "\n",
    "    question: {question}\n",
    "    context: {context}\n",
    "    answer: {answer}\n",
    "    verdict: \"\"\"\n",
    "    client = OpenAI()\n",
    "\n",
    "    prompt = context_precision_prompt.format(\n",
    "        question=question,\n",
    "        context=model_output['context'],\n",
    "        answer=model_output['answer'],\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Changed to gpt-4o\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    response = json.loads(response_message.content)\n",
    "    return {\n",
    "        \"verdict\": int(response[\"verdict\"]) == 1,\n",
    "    }\n",
    "\n",
    "evaluation = weave.Evaluation(dataset=questions, scorers=[context_precision_score])\n",
    "\n",
    "async def run_evaluation():\n",
    "    results = await evaluation.evaluate(model)\n",
    "    print(results)\n",
    "\n",
    "asyncio.run(run_evaluation())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
